{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17825ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda3/envs/rag/lib/python3.13/site-packages/networkx/utils/decorators.py:793: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  func.__dict__.update(f.__dict__)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Rerank is enabled but no rerank_model_func provided. Reranking will be skipped.\n",
      "OpenAI API Call Failed,\n",
      "Model: gpt-4o-mini,\n",
      "Params: {}, Got: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "limit_async: Error in decorated function: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rag.process_document_complete(\n\u001b[32m     88\u001b[39m     file_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data/Benefit_Options.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     89\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./output\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     90\u001b[39m     parse_method=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m )\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Query the processed content\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Pure text query - for basic knowledge base search\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m text_result = \u001b[38;5;28;01mawait\u001b[39;00m rag.aquery(\n\u001b[32m     96\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat are the main findings shown in the figures and tables?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     97\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m )\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mText query result:\u001b[39m\u001b[33m\"\u001b[39m, text_result)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Multimodal query with specific multimodal content\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/raganything/query.py:42\u001b[39m, in \u001b[36mQueryMixin.aquery\u001b[39m\u001b[34m(self, query, mode, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Call LightRAG's query method\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lightrag.aquery(query, param=query_param)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33m\"\u001b[39m\u001b[33mText query completed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/lightrag/lightrag.py:1543\u001b[39m, in \u001b[36mLightRAG.aquery\u001b[39m\u001b[34m(self, query, param, system_prompt)\u001b[39m\n\u001b[32m   1540\u001b[39m param.original_query = query\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m param.mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mglobal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmix\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m kg_query(\n\u001b[32m   1544\u001b[39m         query.strip(),\n\u001b[32m   1545\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph,\n\u001b[32m   1546\u001b[39m         \u001b[38;5;28mself\u001b[39m.entities_vdb,\n\u001b[32m   1547\u001b[39m         \u001b[38;5;28mself\u001b[39m.relationships_vdb,\n\u001b[32m   1548\u001b[39m         \u001b[38;5;28mself\u001b[39m.text_chunks,\n\u001b[32m   1549\u001b[39m         param,\n\u001b[32m   1550\u001b[39m         global_config,\n\u001b[32m   1551\u001b[39m         hashing_kv=\u001b[38;5;28mself\u001b[39m.llm_response_cache,\n\u001b[32m   1552\u001b[39m         system_prompt=system_prompt,\n\u001b[32m   1553\u001b[39m         chunks_vdb=\u001b[38;5;28mself\u001b[39m.chunks_vdb,\n\u001b[32m   1554\u001b[39m     )\n\u001b[32m   1555\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m param.mode == \u001b[33m\"\u001b[39m\u001b[33mnaive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1556\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m naive_query(\n\u001b[32m   1557\u001b[39m         query.strip(),\n\u001b[32m   1558\u001b[39m         \u001b[38;5;28mself\u001b[39m.chunks_vdb,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1562\u001b[39m         system_prompt=system_prompt,\n\u001b[32m   1563\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/lightrag/operate.py:1625\u001b[39m, in \u001b[36mkg_query\u001b[39m\u001b[34m(query, knowledge_graph_inst, entities_vdb, relationships_vdb, text_chunks_db, query_param, global_config, hashing_kv, system_prompt, chunks_vdb)\u001b[39m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_response\n\u001b[32m-> \u001b[39m\u001b[32m1625\u001b[39m hl_keywords, ll_keywords = \u001b[38;5;28;01mawait\u001b[39;00m get_keywords_from_query(\n\u001b[32m   1626\u001b[39m     query, query_param, global_config, hashing_kv\n\u001b[32m   1627\u001b[39m )\n\u001b[32m   1629\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHigh-level keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhl_keywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1630\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLow-level  keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mll_keywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/lightrag/operate.py:1761\u001b[39m, in \u001b[36mget_keywords_from_query\u001b[39m\u001b[34m(query, query_param, global_config, hashing_kv)\u001b[39m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m query_param.hl_keywords, query_param.ll_keywords\n\u001b[32m   1760\u001b[39m \u001b[38;5;66;03m# Extract keywords using extract_keywords_only function which already supports conversation history\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1761\u001b[39m hl_keywords, ll_keywords = \u001b[38;5;28;01mawait\u001b[39;00m extract_keywords_only(\n\u001b[32m   1762\u001b[39m     query, query_param, global_config, hashing_kv\n\u001b[32m   1763\u001b[39m )\n\u001b[32m   1764\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hl_keywords, ll_keywords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/lightrag/operate.py:1833\u001b[39m, in \u001b[36mextract_keywords_only\u001b[39m\u001b[34m(text, param, global_config, hashing_kv)\u001b[39m\n\u001b[32m   1830\u001b[39m     \u001b[38;5;66;03m# Apply higher priority (5) to query relation LLM function\u001b[39;00m\n\u001b[32m   1831\u001b[39m     use_model_func = partial(use_model_func, _priority=\u001b[32m5\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1833\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m use_model_func(kw_prompt, keyword_extraction=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1835\u001b[39m \u001b[38;5;66;03m# 6. Parse out JSON from the LLM response\u001b[39;00m\n\u001b[32m   1836\u001b[39m result = remove_think_tags(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/lightrag/utils.py:610\u001b[39m, in \u001b[36mpriority_limit_async_func_call.<locals>.final_decro.<locals>.wait_func\u001b[39m\u001b[34m(_priority, _timeout, _queue_timeout, *args, **kwargs)\u001b[39m\n\u001b[32m    605\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    606\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlimit_async: Task timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_timeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    607\u001b[39m             )\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    609\u001b[39m         \u001b[38;5;66;03m# Wait for the result without timeout\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    612\u001b[39m     \u001b[38;5;66;03m# Clean up the future reference\u001b[39;00m\n\u001b[32m    613\u001b[39m     active_futures.discard(future)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/lightrag/utils.py:394\u001b[39m, in \u001b[36mpriority_limit_async_func_call.<locals>.final_decro.<locals>.worker\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    393\u001b[39m     \u001b[38;5;66;03m# Execute function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# If future is not done, set the result\u001b[39;00m\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/lightrag/llm/openai.py:188\u001b[39m, in \u001b[36mopenai_complete_if_cache\u001b[39m\u001b[34m(model, prompt, system_prompt, history_messages, base_url, api_key, token_tracker, **kwargs)\u001b[39m\n\u001b[32m    184\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m openai_async_client.beta.chat.completions.parse(\n\u001b[32m    185\u001b[39m             model=model, messages=messages, **kwargs\n\u001b[32m    186\u001b[39m         )\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m openai_async_client.chat.completions.create(\n\u001b[32m    189\u001b[39m             model=model, messages=messages, **kwargs\n\u001b[32m    190\u001b[39m         )\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIConnectionError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    192\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOpenAI API Connection Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2454\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2452\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2453\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2456\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2457\u001b[39m             {\n\u001b[32m   2458\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2461\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2462\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2463\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2465\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2466\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2467\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2468\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2469\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2470\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2471\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2472\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2473\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2475\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2476\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2477\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2478\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2479\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2480\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2481\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2482\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2483\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2489\u001b[39m             },\n\u001b[32m   2490\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2491\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2492\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2493\u001b[39m         ),\n\u001b[32m   2494\u001b[39m         options=make_request_options(\n\u001b[32m   2495\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2496\u001b[39m         ),\n\u001b[32m   2497\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2498\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2499\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/openai/_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag/lib/python3.13/site-packages/openai/_base_client.py:1591\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1588\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1590\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1591\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from raganything import RAGAnything, RAGAnythingConfig\n",
    "from lightrag.llm.openai import openai_complete_if_cache, openai_embed\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "# Create RAGAnything configuration\n",
    "config = RAGAnythingConfig(\n",
    "    working_dir=\"./rag_storage\",\n",
    "    mineru_parse_method=\"auto\",\n",
    "    enable_image_processing=True,\n",
    "    enable_table_processing=True,\n",
    "    enable_equation_processing=True,\n",
    ")\n",
    "\n",
    "# Define LLM model function\n",
    "def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):\n",
    "    return openai_complete_if_cache(\n",
    "        \"deepseek-chat\",\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        api_key=api_key,\n",
    "        base_url=base_url,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "# Define vision model function for image processing\n",
    "def vision_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs\n",
    "):\n",
    "    if image_data:\n",
    "        return openai_complete_if_cache(\n",
    "            \"gpt-4o\",\n",
    "            \"\",\n",
    "            system_prompt=None,\n",
    "            history_messages=[],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt}\n",
    "                if system_prompt\n",
    "                else None,\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{image_data}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "                if image_data\n",
    "                else {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            api_key=api_key,\n",
    "            base_url=base_url,\n",
    "            **kwargs,\n",
    "        )\n",
    "    else:\n",
    "        return llm_model_func(prompt, system_prompt, history_messages, **kwargs)\n",
    "\n",
    "# Define embedding function\n",
    "embedding_func = EmbeddingFunc(\n",
    "    embedding_dim=3072,\n",
    "    max_token_size=8192,\n",
    "    func=lambda texts: openai_embed(\n",
    "        texts,\n",
    "        model=\"text-embedding-3-large\",\n",
    "        api_key=api_key,\n",
    "        base_url=base_url,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Initialize RAGAnything\n",
    "rag = RAGAnything(\n",
    "    config=config,\n",
    "    llm_model_func=llm_model_func,\n",
    "    vision_model_func=vision_model_func,\n",
    "    embedding_func=embedding_func,\n",
    ")\n",
    "\n",
    "# Process a document\n",
    "await rag.process_document_complete(\n",
    "    file_path=\"demo_data/Benefit_Options.pdf\",\n",
    "    output_dir=\"./output\",\n",
    "    parse_method=\"auto\"\n",
    ")\n",
    "\n",
    "# Query the processed content\n",
    "# Pure text query - for basic knowledge base search\n",
    "text_result = await rag.aquery(\n",
    "    \"What are the main findings shown in the figures and tables?\",\n",
    "    mode=\"hybrid\"\n",
    ")\n",
    "print(\"Text query result:\", text_result)\n",
    "\n",
    "# Multimodal query with specific multimodal content\n",
    "multimodal_result = await rag.aquery_with_multimodal(\n",
    "\"Explain this formula and its relevance to the document content\",\n",
    "multimodal_content=[{\n",
    "    \"type\": \"equation\",\n",
    "    \"latex\": \"P(d|q) = \\\\frac{P(q|d) \\\\cdot P(d)}{P(q)}\",\n",
    "    \"equation_caption\": \"Document relevance probability\"\n",
    "}],\n",
    "mode=\"hybrid\"\n",
    ")\n",
    "print(\"Multimodal query result:\", multimodal_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba091b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
